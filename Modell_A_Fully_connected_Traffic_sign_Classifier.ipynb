{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modell A: Fully connected-Traffic-sign-Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQZVhyiZZx3m",
        "outputId": "04f003de-b4f4-455d-c30e-26c5596f1ddc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNRXE5aUaBi5",
        "outputId": "86171056-fae2-469a-a012-fa2ee0a5f229"
      },
      "source": [
        "#packages\n",
        "import pickle\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from pandas.io.parsers import read_csv\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "#specify version of tensoflow!\n",
        "%tensorflow_version 1.15.0\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7AuPVX_a8Mw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppeSUdzFalSQ"
      },
      "source": [
        "#load data\n",
        "\n",
        "training_file = '/content/drive/MyDrive/Colab Notebooks/Datasets/train.p'\n",
        "validation_file='/content/drive/MyDrive/Colab Notebooks/Datasets/valid.p'\n",
        "testing_file = '/content/drive/MyDrive/Colab Notebooks/Datasets/test.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    train = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    valid = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    test = pickle.load(f)\n",
        "    \n",
        "#sign_names = read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/sign_names.csv\").values[:, 0]\n",
        "\n",
        "#uncomment this if you want to train on the original data! but I have now augumented Data!\n",
        "X_train_orig, Y_train_orig = train['features'], train['labels']\n",
        "X_valid, y_valid = valid['features'], valid['labels']\n",
        "X_test_orig, Y_test_orig = test['features'], test['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaSf_o1ta-dY"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOIIKlJia2Bw",
        "outputId": "4cc102fa-7559-4601-a148-d9b6d7a2a754"
      },
      "source": [
        "# Flatten the training and test images\n",
        "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
        "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
        "# Normalize image vectors\n",
        "X_train = X_train_flatten/255.\n",
        "X_test = X_test_flatten/255.\n",
        "# Convert training and test labels to one hot matrices\n",
        "Y_train = convert_to_one_hot(Y_train_orig, 43)\n",
        "Y_test = convert_to_one_hot(Y_test_orig, 43)\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 34799\n",
            "number of test examples = 12630\n",
            "X_train shape: (3072, 34799)\n",
            "Y_train shape: (43, 34799)\n",
            "X_test shape: (3072, 12630)\n",
            "Y_test shape: (43, 12630)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1kLIdceaiDQ"
      },
      "source": [
        "# create_placeholders\n",
        "\n",
        "def create_placeholders(n_x, n_y):\n",
        "    \"\"\"\n",
        "    Creates the placeholders for the tensorflow session.\n",
        "    \n",
        "    Arguments:\n",
        "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
        "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
        "    \n",
        "    Returns:\n",
        "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"tf.float32\"\n",
        "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"tf.float32\"\n",
        "    \n",
        "    Tips:\n",
        "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
        "      In fact, the number of examples during test/train is different.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (approx. 2 lines)\n",
        "    X = tf.placeholder(shape = [n_x, None], dtype = tf.float32, name = 'X')\n",
        "    Y = tf.placeholder(shape = [n_y, None], dtype = tf.float32, name = 'Y')\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmcBEARMbK7R"
      },
      "source": [
        "# initialize_parameters\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
        "                        W1 : [25, 3072]\n",
        "                        b1 : [25, 1]\n",
        "                        W2 : [12, 25]\n",
        "                        b2 : [12, 1]\n",
        "                        W3 : [43, 12]\n",
        "                        b3 : [43, 1]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
        "    \"\"\"\n",
        "    \n",
        "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
        "    #tf.random.set_seed(1)    \n",
        "    ### START CODE HERE ### (approx. 6 lines of code)\n",
        "    W1 = tf.get_variable(\"W1\", [25,3072], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
        "    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
        "    W3 = tf.get_variable(\"W3\", [43,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b3 = tf.get_variable(\"b3\", [43,1], initializer = tf.zeros_initializer())\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"W3\": W3,\n",
        "                  \"b3\": b3}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a1_YAGobOY5"
      },
      "source": [
        "# forward_propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)                    # Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tf.nn.relu(Z1)                                  # A1 = relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)                   # Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = tf.nn.relu(Z2)                                  # A2 = relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)                   # Z3 = np.dot(W3, A2) + b3\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return Z3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDD2EXN2bRlQ"
      },
      "source": [
        "# compute_cost \n",
        "\n",
        "def compute_cost(Z3, Y):\n",
        "    \"\"\"\n",
        "    Computes the cost\n",
        "    \n",
        "    Arguments:\n",
        "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
        "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
        "    \n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    \"\"\"\n",
        "    \n",
        "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
        "    logits = tf.transpose(Z3)\n",
        "    labels = tf.transpose(Y)\n",
        "    \n",
        "    ### START CODE HERE ### (1 line of code)\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D83lxlMbbVRo"
      },
      "source": [
        "#build the model!\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
        "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
        "    \"\"\"\n",
        "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
        "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
        "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
        "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_epochs -- number of epochs of the optimization loop\n",
        "    minibatch_size -- size of a minibatch\n",
        "    print_cost -- True to print the cost every 100 epochs\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
        "    tf.set_random_seed(1)                             # to keep consistent results\n",
        "    #tf.random.set_seed(1)\n",
        "    seed = 3                                          # to keep consistent results\n",
        "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
        "    n_y = Y_train.shape[0]                            # n_y : output size\n",
        "    costs = []                                        # To keep track of the cost\n",
        "    \n",
        "    # Create Placeholders of shape (n_x, n_y)\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    X, Y = create_placeholders(n_x, n_y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    parameters = initialize_parameters()\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    Z3 = forward_propagation(X, parameters)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Cost function: Add cost function to tensorflow graph\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    cost = compute_cost(Z3, Y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Initialize all the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    num_examples = len(X_train)\n",
        "    # Start the session to compute the tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        # Run the initialization\n",
        "        sess.run(init)\n",
        "        \n",
        "        # Do the training loop\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
        "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "            seed = seed + 1\n",
        "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
        "            # X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "            # for offset in range(0, num_examples, minibatch_size):\n",
        "            #     end = offset + minibatch_size\n",
        "            #     batch_x, batch_y = X_train[offset:end], Y_train[offset:end]\n",
        "\n",
        "            #     _ , minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: batch_x, Y: batch_y})\n",
        "                \n",
        "            #     epoch_cost += minibatch_cost / minibatch_size\n",
        "\n",
        "            for minibatch in minibatches:   \n",
        "\n",
        "                # Select a minibatch\n",
        "                (minibatch_X, minibatch_Y) = minibatch\n",
        "                \n",
        "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
        "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
        "                ### START CODE HERE ### (1 line)\n",
        "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: minibatch_X, Y: minibatch_Y})\n",
        "                ### END CODE HERE ###\n",
        "                \n",
        "                epoch_cost += minibatch_cost / minibatch_size\n",
        "\n",
        "            # Print the cost every epoch\n",
        "            if print_cost == True and epoch % 100 == 0:\n",
        "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "            if print_cost == True and epoch % 5 == 0:\n",
        "                costs.append(epoch_cost)\n",
        "                \n",
        "        # plot the cost\n",
        "        plt.plot(np.squeeze(costs))\n",
        "        plt.ylabel('cost')\n",
        "        plt.xlabel('iterations (per fives)')\n",
        "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "        plt.show()\n",
        "\n",
        "        # lets save the parameters in a variable\n",
        "        parameters = sess.run(parameters)\n",
        "        print (\"Parameters have been trained!\")\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
        "\n",
        "        # Calculate accuracy on the test set\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
        "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
        "        \n",
        "        return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "ErFuLo-cbgVC",
        "outputId": "d9bc3b9e-ff2f-43e5-8a53-bc70699d88ba"
      },
      "source": [
        "parameters = model(X_train, Y_train, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-10-33958e0deee3>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Cost after epoch 0: 117.613657\n",
            "Cost after epoch 100: 9.238440\n",
            "Cost after epoch 200: 5.613389\n",
            "Cost after epoch 300: 3.906583\n",
            "Cost after epoch 400: 2.805101\n",
            "Cost after epoch 500: 2.088295\n",
            "Cost after epoch 600: 1.607830\n",
            "Cost after epoch 700: 1.220599\n",
            "Cost after epoch 800: 0.993754\n",
            "Cost after epoch 900: 0.869027\n",
            "Cost after epoch 1000: 0.646392\n",
            "Cost after epoch 1100: 0.493466\n",
            "Cost after epoch 1200: 0.603938\n",
            "Cost after epoch 1300: 0.339028\n",
            "Cost after epoch 1400: 0.239839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Parameters have been trained!\n",
            "Train Accuracy: 0.99971265\n",
            "Test Accuracy: 0.8293745\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}