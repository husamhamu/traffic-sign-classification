{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flly connected with Aug3000 - traffic sign classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('tf': conda)",
      "name": "python3710jvsc74a57bd00ec74a707f3534fe3058fe7f26c045381fddc9215353e90c7b754d9cb5d0b0fd"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-q2uXOreWgH",
        "outputId": "854b5ce9-e778-41e4-bbc0-4eab6c14db8e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlONXqisd_TT",
        "outputId": "9bc34a78-ed9e-43ca-de0f-36ec2ced2123"
      },
      "source": [
        "#packages\n",
        "import pickle\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from pandas.io.parsers import read_csv\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "#specify version of tensoflow!\n",
        "%tensorflow_version 1.15.0\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1qobNmrd_TL"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yddr92qd_TU"
      },
      "source": [
        "#load data\n",
        "\n",
        "training_file = '/content/drive/MyDrive/Colab Notebooks/Datasets/train.p'\n",
        "validation_file='/content/drive/MyDrive/Colab Notebooks/Datasets/valid.p'\n",
        "testing_file = '/content/drive/MyDrive/Colab Notebooks/Datasets/test.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    train = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    valid = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    test = pickle.load(f)\n",
        "    \n",
        "#sign_names = read_csv(\"/content/drive/MyDrive/Colab Notebooks/Datasets/sign_names.csv\").values[:, 0]\n",
        "\n",
        "#uncomment this if you want to train on the original data! but I have now augumented Data!\n",
        "X_train_orig, Y_train_orig = train['features'], train['labels']\n",
        "X_valid, y_valid = valid['features'], valid['labels']\n",
        "X_test_orig, Y_test_orig = test['features'], test['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn9Qf6T3d_TU",
        "outputId": "56fe62fd-e5a1-41cc-da48-a425fee9376f"
      },
      "source": [
        "# Flatten the training and test images\n",
        "#X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
        "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
        "\n",
        "# Normalize image vectors\n",
        "#X_train = X_train_flatten/255.\n",
        "X_test = X_test_flatten/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "Y_train = convert_to_one_hot(Y_train_orig, 43)\n",
        "Y_test = convert_to_one_hot(Y_test_orig, 43)\n",
        "\n",
        "#print (\"number of training examples = \" + str(X_train.shape[1]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
        "#print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of test examples = 12630\n",
            "Y_train shape: (43, 34799)\n",
            "X_test shape: (3072, 12630)\n",
            "Y_test shape: (43, 12630)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz6T5YX0d_TV"
      },
      "source": [
        "#  create_placeholders\n",
        "\n",
        "def create_placeholders(n_x, n_y):\n",
        "    \"\"\"\n",
        "    Creates the placeholders for the tensorflow session.\n",
        "    \n",
        "    Arguments:\n",
        "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
        "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
        "    \n",
        "    Returns:\n",
        "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"tf.float32\"\n",
        "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"tf.float32\"\n",
        "    \n",
        "    Tips:\n",
        "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
        "      In fact, the number of examples during test/train is different.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (approx. 2 lines)\n",
        "    X = tf.placeholder(shape = [n_x, None], dtype = tf.float32, name = 'X')\n",
        "    Y = tf.placeholder(shape = [n_y, None], dtype = tf.float32, name = 'Y')\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "619xJnu2d_TW"
      },
      "source": [
        "# initialize_parameters\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
        "                        W1 : [25, 3072]\n",
        "                        b1 : [25, 1]\n",
        "                        W2 : [12, 25]\n",
        "                        b2 : [12, 1]\n",
        "                        W3 : [43, 12]\n",
        "                        b3 : [43, 1]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
        "    \"\"\"\n",
        "    \n",
        "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
        "    #tf.random.set_seed(1)    \n",
        "    ### START CODE HERE ### (approx. 6 lines of code)\n",
        "    W1 = tf.get_variable(\"W1\", [30,3072], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b1 = tf.get_variable(\"b1\", [30,1], initializer = tf.zeros_initializer())\n",
        "    W2 = tf.get_variable(\"W2\", [12,30], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
        "    W3 = tf.get_variable(\"W3\", [43,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    b3 = tf.get_variable(\"b3\", [43,1], initializer = tf.zeros_initializer())\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2,\n",
        "                  \"W3\": W3,\n",
        "                  \"b3\": b3}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THbjT9I-d_TW"
      },
      "source": [
        "# forward_propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3']\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)                    # Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tf.nn.relu(Z1)                                  # A1 = relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)                   # Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = tf.nn.relu(Z2)                                  # A2 = relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)                   # Z3 = np.dot(W3, A2) + b3\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return Z3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K-rrLA5d_TX"
      },
      "source": [
        "#  compute_cost \n",
        "\n",
        "def compute_cost(Z3, Y):\n",
        "    \"\"\"\n",
        "    Computes the cost\n",
        "    \n",
        "    Arguments:\n",
        "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
        "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
        "    \n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    \"\"\"\n",
        "    \n",
        "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
        "    logits = tf.transpose(Z3)\n",
        "    labels = tf.transpose(Y)\n",
        "    \n",
        "    ### START CODE HERE ### (1 line of code)\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B-XbUr7d_TX"
      },
      "source": [
        "#build the model!\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
        "          num_epochs = 1500, minibatch_size = 64, print_cost = True):\n",
        "    \"\"\"\n",
        "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
        "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
        "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
        "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_epochs -- number of epochs of the optimization loop\n",
        "    minibatch_size -- size of a minibatch\n",
        "    print_cost -- True to print the cost every 100 epochs\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
        "    tf.set_random_seed(1)                             # to keep consistent results\n",
        "    #tf.random.set_seed(1)\n",
        "    seed = 3                                          # to keep consistent results\n",
        "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
        "    n_y = Y_train.shape[0]                            # n_y : output size\n",
        "    costs = []                                        # To keep track of the cost\n",
        "    \n",
        "    # Create Placeholders of shape (n_x, n_y)\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    X, Y = create_placeholders(n_x, n_y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    parameters = initialize_parameters()\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    Z3 = forward_propagation(X, parameters)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Cost function: Add cost function to tensorflow graph\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    cost = compute_cost(Z3, Y)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
        "    ### START CODE HERE ### (1 line)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Initialize all the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    num_examples = len(X_train)\n",
        "    # Start the session to compute the tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        # Run the initialization\n",
        "        sess.run(init)\n",
        "        \n",
        "        # Do the training loop\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
        "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "            seed = seed + 1\n",
        "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
        "            # X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "            # for offset in range(0, num_examples, minibatch_size):\n",
        "            #     end = offset + minibatch_size\n",
        "            #     batch_x, batch_y = X_train[offset:end], Y_train[offset:end]\n",
        "\n",
        "            #     _ , minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: batch_x, Y: batch_y})\n",
        "                \n",
        "            #     epoch_cost += minibatch_cost / minibatch_size\n",
        "\n",
        "            for minibatch in minibatches:   \n",
        "\n",
        "                # Select a minibatch\n",
        "                (minibatch_X, minibatch_Y) = minibatch\n",
        "                \n",
        "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
        "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
        "                ### START CODE HERE ### (1 line)\n",
        "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: minibatch_X, Y: minibatch_Y})\n",
        "                ### END CODE HERE ###\n",
        "                \n",
        "                epoch_cost += minibatch_cost / minibatch_size\n",
        "\n",
        "            # Print the cost every epoch\n",
        "            if print_cost == True and epoch % 100 == 0:\n",
        "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "            if print_cost == True and epoch % 5 == 0:\n",
        "                costs.append(epoch_cost)\n",
        "                \n",
        "        # plot the cost\n",
        "        plt.plot(np.squeeze(costs))\n",
        "        plt.ylabel('cost')\n",
        "        plt.xlabel('iterations (per fives)')\n",
        "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "        plt.show()\n",
        "\n",
        "        # lets save the parameters in a variable\n",
        "        parameters = sess.run(parameters)\n",
        "        print (\"Parameters have been trained!\")\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
        "\n",
        "        # Calculate accuracy on the test set\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
        "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
        "        \n",
        "        return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5XalXn3d_Tb",
        "outputId": "c51e1c16-f718-4888-86ee-03898d941398"
      },
      "source": [
        "##Load premade augmented data\n",
        "\n",
        "augmented_file = '/content/drive/MyDrive/Colab Notebooks/Datasets/augmentedData3000.npz'\n",
        "\n",
        "loaded = np.load(augmented_file)\n",
        "X_train_aug_orig = loaded['X_train_aug']\n",
        "y_train_aug_orig = loaded['y_train_aug']\n",
        "\n",
        "print(\"Augmented Dataset size X\")\n",
        "print(X_train_aug_orig.shape)\n",
        "print(\"Augmented Dataset size y\")\n",
        "print(y_train_aug_orig.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Augmented Dataset size X\n",
            "(129000, 32, 32, 3)\n",
            "Augmented Dataset size y\n",
            "(129000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpuu9jHpd_Tb",
        "outputId": "b5f6d864-dd1d-4671-a4c8-38c171de1e46"
      },
      "source": [
        "# Flatten the training and test images\n",
        "X_train_aug_orig_flatten = X_train_aug_orig.reshape(X_train_aug_orig.shape[0], -1).T\n",
        "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
        "# Normalize image vectors\n",
        "X_train_aug = X_train_aug_orig_flatten/255.\n",
        "X_test = X_test_flatten/255.\n",
        "# Convert training and test labels to one hot matrices\n",
        "Y_train_aug = convert_to_one_hot(y_train_aug_orig, 43)\n",
        "Y_test = convert_to_one_hot(Y_test_orig, 43)\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train_aug.shape[1]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
        "print (\"X_train shape: \" + str(X_train_aug.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train_aug.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 129000\n",
            "number of test examples = 12630\n",
            "X_train shape: (3072, 129000)\n",
            "Y_train shape: (43, 129000)\n",
            "X_test shape: (3072, 12630)\n",
            "Y_test shape: (43, 12630)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "jQ6m_M5_d_Tc",
        "outputId": "85c9cf34-00be-4db9-ecd6-17f6b5200823"
      },
      "source": [
        "parameters = model(X_train_aug, Y_train_aug, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-11-e1fb6cef6ed2>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Cost after epoch 0: 109.947006\n",
            "Cost after epoch 100: 42.313779\n",
            "Cost after epoch 200: 38.086606\n",
            "Cost after epoch 300: 36.067920\n",
            "Cost after epoch 400: 34.678854\n",
            "Cost after epoch 500: 33.553555\n",
            "Cost after epoch 600: 32.401875\n",
            "Cost after epoch 700: 31.552409\n",
            "Cost after epoch 800: 30.917457\n",
            "Cost after epoch 900: 30.431762\n",
            "Cost after epoch 1000: 29.959849\n",
            "Cost after epoch 1100: 29.580272\n",
            "Cost after epoch 1200: 29.221350\n",
            "Cost after epoch 1300: 28.915888\n",
            "Cost after epoch 1400: 28.625551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Parameters have been trained!\n",
            "Train Accuracy: 0.7449845\n",
            "Test Accuracy: 0.731829\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}